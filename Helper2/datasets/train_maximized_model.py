#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ —É–ª—É—á—à–µ–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ
"""

import os
import shutil
from pathlib import Path
import json

def prepare_maximized_dataset():
    """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —É–ª—É—á—à–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    print("üîÑ –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —É–ª—É—á—à–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç...")
    
    # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    train_dir = Path("maximized_training_data")
    train_dir.mkdir(exist_ok=True)
    
    # –°–æ–∑–¥–∞–µ–º –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    (train_dir / "train" / "positive").mkdir(parents=True, exist_ok=True)
    (train_dir / "train" / "negative").mkdir(parents=True, exist_ok=True)
    (train_dir / "val" / "positive").mkdir(parents=True, exist_ok=True)
    (train_dir / "val" / "negative").mkdir(parents=True, exist_ok=True)
    (train_dir / "test" / "positive").mkdir(parents=True, exist_ok=True)
    (train_dir / "test" / "negative").mkdir(parents=True, exist_ok=True)
    
    # –ö–æ–ø–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ maximized_dataset
    source_positive = Path("maximized_dataset/positive")
    source_negative = Path("maximized_dataset/negative")
    
    if source_positive.exists():
        positive_images = list(source_positive.glob("*.jpg"))
        negative_images = list(source_negative.glob("*.jpg"))
        
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:")
        print(f"   –î–µ—Ñ–µ–∫—Ç—ã: {len(positive_images)}")
        print(f"   –ù–æ—Ä–º–∞: {len(negative_images)}")
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/val/test (70%/15%/15%)
        train_pos = positive_images[:int(len(positive_images) * 0.7)]
        val_pos = positive_images[int(len(positive_images) * 0.7):int(len(positive_images) * 0.85)]
        test_pos = positive_images[int(len(positive_images) * 0.85):]
        
        train_neg = negative_images[:int(len(negative_images) * 0.7)]
        val_neg = negative_images[int(len(negative_images) * 0.7):int(len(negative_images) * 0.85)]
        test_neg = negative_images[int(len(negative_images) * 0.85):]
        
        # –ö–æ–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã
        for img in train_pos:
            shutil.copy2(img, train_dir / "train" / "positive" / img.name)
        for img in val_pos:
            shutil.copy2(img, train_dir / "val" / "positive" / img.name)
        for img in test_pos:
            shutil.copy2(img, train_dir / "test" / "positive" / img.name)
            
        for img in train_neg:
            shutil.copy2(img, train_dir / "train" / "negative" / img.name)
        for img in val_neg:
            shutil.copy2(img, train_dir / "val" / "negative" / img.name)
        for img in test_neg:
            shutil.copy2(img, train_dir / "test" / "negative" / img.name)
        
        print(f"‚úÖ –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ:")
        print(f"   Train: {len(train_pos)} –¥–µ—Ñ–µ–∫—Ç–æ–≤, {len(train_neg)} –Ω–æ—Ä–º–∞")
        print(f"   Val: {len(val_pos)} –¥–µ—Ñ–µ–∫—Ç–æ–≤, {len(val_neg)} –Ω–æ—Ä–º–∞")
        print(f"   Test: {len(test_pos)} –¥–µ—Ñ–µ–∫—Ç–æ–≤, {len(test_neg)} –Ω–æ—Ä–º–∞")
    
    return train_dir

def create_maximized_config():
    """–°–æ–∑–¥–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    config = {
        "dataset_name": "maximized_concrete_defects",
        "description": "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —É–ª—É—á—à–µ–Ω–Ω—ã–π —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–µ—Ñ–µ–∫—Ç–æ–≤ –±–µ—Ç–æ–Ω–∞",
        "classes": {
            "positive": "–î–µ—Ñ–µ–∫—Ç—ã (—Ç—Ä–µ—â–∏–Ω—ã, –ø—è—Ç–Ω–∞, –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏—è, —Å–∫–æ–ª—ã)",
            "negative": "–ù–æ—Ä–º–∞–ª—å–Ω—ã–π –±–µ—Ç–æ–Ω –±–µ–∑ –¥–µ—Ñ–µ–∫—Ç–æ–≤"
        },
        "image_size": [224, 224],
        "augmentation": {
            "rotation": 30,
            "brightness": 0.4,
            "contrast": 0.4,
            "horizontal_flip": True,
            "vertical_flip": True,
            "zoom_range": 0.3,
            "shear_range": 0.2,
            "width_shift_range": 0.2,
            "height_shift_range": 0.2
        },
        "training": {
            "batch_size": 32,
            "epochs": 50,
            "learning_rate": 0.0001,
            "validation_split": 0.2,
            "class_weight": {
                "0": 1.0,  # negative
                "1": 1.0   # positive (—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–æ)
            }
        },
        "model": {
            "architecture": "advanced_cnn",
            "regularization": "strong",
            "optimization": "adam_with_scheduling"
        }
    }
    
    return config

def create_maximized_training_script():
    """–°–æ–∑–¥–∞–µ—Ç —Å–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    script_content = '''#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ —É–ª—É—á—à–µ–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import json
from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt

def load_dataset_config():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    with open("maximized_dataset_config.json", "r", encoding="utf-8") as f:
        return json.load(f)

def create_advanced_data_generators(config):
    """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö"""
    train_datagen = keras.preprocessing.image.ImageDataGenerator(
        rescale=1./255,
        rotation_range=config["augmentation"]["rotation"],
        brightness_range=[1-config["augmentation"]["brightness"], 
                         1+config["augmentation"]["brightness"]],
        contrast_range=[1-config["augmentation"]["contrast"], 
                       1+config["augmentation"]["contrast"]],
        horizontal_flip=config["augmentation"]["horizontal_flip"],
        vertical_flip=config["augmentation"]["vertical_flip"],
        zoom_range=config["augmentation"]["zoom_range"],
        shear_range=config["augmentation"]["shear_range"],
        width_shift_range=config["augmentation"]["width_shift_range"],
        height_shift_range=config["augmentation"]["height_shift_range"],
        fill_mode='nearest'
    )
    
    train_generator = train_datagen.flow_from_directory(
        "train/",
        target_size=tuple(config["image_size"]),
        batch_size=config["training"]["batch_size"],
        class_mode="binary",
        shuffle=True
    )
    
    val_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)
    val_generator = val_datagen.flow_from_directory(
        "val/",
        target_size=tuple(config["image_size"]),
        batch_size=config["training"]["batch_size"],
        class_mode="binary",
        shuffle=False
    )
    
    return train_generator, val_generator

def create_maximized_model(config):
    """–°–æ–∑–¥–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω—É—é –º–æ–¥–µ–ª—å"""
    model = keras.Sequential([
        # –ü–µ—Ä–≤—ã–π –±–ª–æ–∫
        layers.Conv2D(32, (3, 3), activation="relu", input_shape=(*config["image_size"], 3)),
        layers.BatchNormalization(),
        layers.Conv2D(32, (3, 3), activation="relu"),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.25),
        
        # –í—Ç–æ—Ä–æ–π –±–ª–æ–∫
        layers.Conv2D(64, (3, 3), activation="relu"),
        layers.BatchNormalization(),
        layers.Conv2D(64, (3, 3), activation="relu"),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.25),
        
        # –¢—Ä–µ—Ç–∏–π –±–ª–æ–∫
        layers.Conv2D(128, (3, 3), activation="relu"),
        layers.BatchNormalization(),
        layers.Conv2D(128, (3, 3), activation="relu"),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.25),
        
        # –ß–µ—Ç–≤–µ—Ä—Ç—ã–π –±–ª–æ–∫
        layers.Conv2D(256, (3, 3), activation="relu"),
        layers.BatchNormalization(),
        layers.Conv2D(256, (3, 3), activation="relu"),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.25),
        
        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
        layers.Flatten(),
        layers.Dense(1024, activation="relu"),
        layers.BatchNormalization(),
        layers.Dropout(0.5),
        layers.Dense(512, activation="relu"),
        layers.BatchNormalization(),
        layers.Dropout(0.5),
        layers.Dense(256, activation="relu"),
        layers.Dropout(0.3),
        layers.Dense(1, activation="sigmoid")
    ])
    
    # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–º
    optimizer = keras.optimizers.Adam(
        learning_rate=config["training"]["learning_rate"],
        beta_1=0.9,
        beta_2=0.999,
        epsilon=1e-07
    )
    
    model.compile(
        optimizer=optimizer,
        loss="binary_crossentropy",
        metrics=["accuracy", "precision", "recall", "f1_score"]
    )
    
    return model

def train_maximized_model():
    """–û–±—É—á–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω—É—é –º–æ–¥–µ–ª—å"""
    print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω–æ–π –º–æ–¥–µ–ª–∏...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config = load_dataset_config()
    
    # –°–æ–∑–¥–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö
    train_gen, val_gen = create_advanced_data_generators(config)
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model = create_maximized_model(config)
    
    # –í—ã–≤–æ–¥–∏–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏
    model.summary()
    
    # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ callbacks
    callbacks = [
        keras.callbacks.EarlyStopping(
            monitor='val_f1_score',
            patience=10,
            restore_best_weights=True,
            mode='max'
        ),
        keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=5,
            min_lr=1e-8,
            verbose=1
        ),
        keras.callbacks.ModelCheckpoint(
            'best_maximized_model.h5',
            monitor='val_f1_score',
            save_best_only=True,
            mode='max',
            verbose=1
        )
    ]
    
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    history = model.fit(
        train_gen,
        epochs=config["training"]["epochs"],
        validation_data=val_gen,
        callbacks=callbacks,
        verbose=1,
        class_weight=config["training"]["class_weight"]
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
    model.save("maximized_concrete_defect_model.h5")
    print("‚úÖ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: maximized_concrete_defect_model.h5")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è
    with open("maximized_training_history.json", "w") as f:
        json.dump({
            'loss': [float(x) for x in history.history['loss']],
            'accuracy': [float(x) for x in history.history['accuracy']],
            'precision': [float(x) for x in history.history['precision']],
            'recall': [float(x) for x in history.history['recall']],
            'f1_score': [float(x) for x in history.history['f1_score']],
            'val_loss': [float(x) for x in history.history['val_loss']],
            'val_accuracy': [float(x) for x in history.history['val_accuracy']],
            'val_precision': [float(x) for x in history.history['val_precision']],
            'val_recall': [float(x) for x in history.history['val_recall']],
            'val_f1_score': [float(x) for x in history.history['val_f1_score']]
        }, f)
    
    # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
    plot_training_history(history)
    
    return model, history

def plot_training_history(history):
    """–°–æ–∑–¥–∞–µ—Ç –≥—Ä–∞—Ñ–∏–∫–∏ –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è"""
    try:
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # –¢–æ—á–Ω–æ—Å—Ç—å
        axes[0, 0].plot(history.history['accuracy'], label='Training')
        axes[0, 0].plot(history.history['val_accuracy'], label='Validation')
        axes[0, 0].set_title('Model Accuracy')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Accuracy')
        axes[0, 0].legend()
        
        # –ü–æ—Ç–µ—Ä–∏
        axes[0, 1].plot(history.history['loss'], label='Training')
        axes[0, 1].plot(history.history['val_loss'], label='Validation')
        axes[0, 1].set_title('Model Loss')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Loss')
        axes[0, 1].legend()
        
        # F1 Score
        axes[1, 0].plot(history.history['f1_score'], label='Training')
        axes[1, 0].plot(history.history['val_f1_score'], label='Validation')
        axes[1, 0].set_title('F1 Score')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('F1 Score')
        axes[1, 0].legend()
        
        # Precision vs Recall
        axes[1, 1].plot(history.history['precision'], label='Precision')
        axes[1, 1].plot(history.history['recall'], label='Recall')
        axes[1, 1].set_title('Precision vs Recall')
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('Score')
        axes[1, 1].legend()
        
        plt.tight_layout()
        plt.savefig('training_history.png', dpi=300, bbox_inches='tight')
        print("üìä –ì—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: training_history.png")
        
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤: {e}")

if __name__ == "__main__":
    train_maximized_model()
'''
    
    return script_content

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –æ–±—É—á–µ–Ω–∏—é –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω–æ–π –º–æ–¥–µ–ª–∏")
    print("=" * 70)
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    train_dir = prepare_maximized_dataset()
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config = create_maximized_config()
    with open(train_dir / "maximized_dataset_config.json", "w", encoding="utf-8") as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    # –°–æ–∑–¥–∞–µ–º —Å–∫—Ä–∏–ø—Ç –æ–±—É—á–µ–Ω–∏—è
    script_content = create_maximized_training_script()
    with open(train_dir / "train_maximized_model.py", "w", encoding="utf-8") as f:
        f.write(script_content)
    
    print(f"\n‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print(f"üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {train_dir}")
    print(f"üìä –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {train_dir}/maximized_dataset_config.json")
    print(f"ü§ñ –°–∫—Ä–∏–ø—Ç –æ–±—É—á–µ–Ω–∏—è: {train_dir}/train_maximized_model.py")
    
    print(f"\nüéØ –î–ª—è –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω–æ–π –º–æ–¥–µ–ª–∏:")
    print(f"   cd {train_dir}")
    print(f"   python train_maximized_model.py")
    
    print(f"\nüìà –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
    print(f"   - –¢–æ—á–Ω–æ—Å—Ç—å: >90%")
    print(f"   - F1 Score: >0.9")
    print(f"   - –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –æ–±–æ–∏—Ö –∫–ª–∞—Å—Å–∞—Ö")
    print(f"   - –í—ã—Å–æ–∫–∞—è –æ–±–æ–±—â–∞—é—â–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å")

if __name__ == "__main__":
    main()


