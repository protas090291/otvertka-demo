#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å —Å–∏—Å—Ç–µ–º–æ–π –∞–Ω–∞–ª–∏–∑–∞ –¥–µ—Ñ–µ–∫—Ç–æ–≤
"""

import os
import shutil
from pathlib import Path
import json

def create_training_structure():
    """–°–æ–∑–¥–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏"""
    print("üîÑ –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...")
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    train_dirs = [
        "training_data/train/positive",
        "training_data/train/negative", 
        "training_data/val/positive",
        "training_data/val/negative",
        "training_data/test/positive",
        "training_data/test/negative"
    ]
    
    for dir_path in train_dirs:
        Path(dir_path).mkdir(parents=True, exist_ok=True)
    
    return train_dirs

def split_dataset():
    """–†–∞–∑–¥–µ–ª—è–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞ train/val/test"""
    print("üîÑ –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞ train/val/test...")
    
    # –ö–æ–ø–∏—Ä—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    source_positive = Path("concrete_cracks/positive")
    source_negative = Path("concrete_cracks/negative")
    
    if source_positive.exists():
        positive_images = list(source_positive.glob("*.jpg"))
        negative_images = list(source_negative.glob("*.jpg"))
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã (80% train, 10% val, 10% test)
        train_pos = positive_images[:int(len(positive_images) * 0.8)]
        val_pos = positive_images[int(len(positive_images) * 0.8):int(len(positive_images) * 0.9)]
        test_pos = positive_images[int(len(positive_images) * 0.9):]
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
        train_neg = negative_images[:int(len(negative_images) * 0.8)]
        val_neg = negative_images[int(len(negative_images) * 0.8):int(len(negative_images) * 0.9)]
        test_neg = negative_images[int(len(negative_images) * 0.9):]
        
        # –ö–æ–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã
        for img in train_pos:
            shutil.copy2(img, "training_data/train/positive/")
        for img in val_pos:
            shutil.copy2(img, "training_data/val/positive/")
        for img in test_pos:
            shutil.copy2(img, "training_data/test/positive/")
            
        for img in train_neg:
            shutil.copy2(img, "training_data/train/negative/")
        for img in val_neg:
            shutil.copy2(img, "training_data/val/negative/")
        for img in test_neg:
            shutil.copy2(img, "training_data/test/negative/")
        
        print(f"‚úÖ –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ:")
        print(f"   Train: {len(train_pos)} –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö, {len(train_neg)} –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö")
        print(f"   Val: {len(val_pos)} –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö, {len(val_neg)} –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö")
        print(f"   Test: {len(test_pos)} –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö, {len(test_neg)} –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö")

def create_dataset_config():
    """–°–æ–∑–¥–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    config = {
        "dataset_name": "concrete_defects",
        "description": "–î–∞—Ç–∞—Å–µ—Ç –¥–µ—Ñ–µ–∫—Ç–æ–≤ –±–µ—Ç–æ–Ω–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏",
        "classes": {
            "positive": "–î–µ—Ñ–µ–∫—Ç—ã (—Ç—Ä–µ—â–∏–Ω—ã, —Å–∫–æ–ª—ã, –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏—è)",
            "negative": "–ù–æ—Ä–º–∞–ª—å–Ω—ã–π –±–µ—Ç–æ–Ω –±–µ–∑ –¥–µ—Ñ–µ–∫—Ç–æ–≤"
        },
        "image_size": [224, 224],
        "augmentation": {
            "rotation": 15,
            "brightness": 0.2,
            "contrast": 0.2,
            "horizontal_flip": True
        },
        "training": {
            "batch_size": 32,
            "epochs": 50,
            "learning_rate": 0.001,
            "validation_split": 0.2
        }
    }
    
    with open("training_data/dataset_config.json", "w", encoding="utf-8") as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    print("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å–æ–∑–¥–∞–Ω–∞: training_data/dataset_config.json")

def create_training_script():
    """–°–æ–∑–¥–∞–µ—Ç —Å–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏"""
    training_script = '''#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–µ—Ñ–µ–∫—Ç–æ–≤ –±–µ—Ç–æ–Ω–∞
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import json
from pathlib import Path
import numpy as np

def load_dataset_config():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    with open("dataset_config.json", "r", encoding="utf-8") as f:
        return json.load(f)

def create_data_generators(config):
    """–°–æ–∑–¥–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    train_datagen = keras.preprocessing.image.ImageDataGenerator(
        rescale=1./255,
        rotation_range=config["augmentation"]["rotation"],
        brightness_range=[1-config["augmentation"]["brightness"], 
                         1+config["augmentation"]["brightness"]],
        horizontal_flip=config["augmentation"]["horizontal_flip"],
        validation_split=config["training"]["validation_split"]
    )
    
    train_generator = train_datagen.flow_from_directory(
        "train/",
        target_size=tuple(config["image_size"]),
        batch_size=config["training"]["batch_size"],
        class_mode="binary",
        subset="training"
    )
    
    val_generator = train_datagen.flow_from_directory(
        "train/",
        target_size=tuple(config["image_size"]),
        batch_size=config["training"]["batch_size"],
        class_mode="binary",
        subset="validation"
    )
    
    return train_generator, val_generator

def create_model(config):
    """–°–æ–∑–¥–∞–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    model = keras.Sequential([
        layers.Conv2D(32, (3, 3), activation="relu", input_shape=(*config["image_size"], 3)),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), activation="relu"),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), activation="relu"),
        layers.Flatten(),
        layers.Dense(64, activation="relu"),
        layers.Dropout(0.5),
        layers.Dense(1, activation="sigmoid")
    ])
    
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=config["training"]["learning_rate"]),
        loss="binary_crossentropy",
        metrics=["accuracy"]
    )
    
    return model

def train_model():
    """–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å"""
    print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config = load_dataset_config()
    
    # –°–æ–∑–¥–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö
    train_gen, val_gen = create_data_generators(config)
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model = create_model(config)
    
    # –í—ã–≤–æ–¥–∏–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏
    model.summary()
    
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    history = model.fit(
        train_gen,
        epochs=config["training"]["epochs"],
        validation_data=val_gen,
        verbose=1
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    model.save("concrete_defect_model.h5")
    print("‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: concrete_defect_model.h5")
    
    return model, history

if __name__ == "__main__":
    train_model()
'''
    
    with open("training_data/train_model.py", "w", encoding="utf-8") as f:
        f.write(training_script)
    
    print("‚úÖ –°–∫—Ä–∏–ø—Ç –æ–±—É—á–µ–Ω–∏—è —Å–æ–∑–¥–∞–Ω: training_data/train_model.py")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å —Å–∏—Å—Ç–µ–º–æ–π –∞–Ω–∞–ª–∏–∑–∞ –¥–µ—Ñ–µ–∫—Ç–æ–≤...")
    
    # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    create_training_structure()
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    split_dataset()
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    create_dataset_config()
    
    # –°–æ–∑–¥–∞–µ–º —Å–∫—Ä–∏–ø—Ç –æ–±—É—á–µ–Ω–∏—è
    create_training_script()
    
    print("‚úÖ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print("üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞:")
    print("   training_data/")
    print("   ‚îú‚îÄ‚îÄ train/")
    print("   ‚îú‚îÄ‚îÄ val/")
    print("   ‚îú‚îÄ‚îÄ test/")
    print("   ‚îú‚îÄ‚îÄ dataset_config.json")
    print("   ‚îî‚îÄ‚îÄ train_model.py")
    print("")
    print("üéØ –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
    print("   1. cd training_data")
    print("   2. pip install tensorflow")
    print("   3. python train_model.py")

if __name__ == "__main__":
    main()


