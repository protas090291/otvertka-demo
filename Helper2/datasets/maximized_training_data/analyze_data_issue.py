#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–ª–µ–º—ã —Å –¥–∞–Ω–Ω—ã–º–∏ - –ø–æ—á–µ–º—É –º–æ–¥–µ–ª—å –Ω–µ –º–æ–∂–µ—Ç —Ä–∞–∑–ª–∏—á–∏—Ç—å –¥–µ—Ñ–µ–∫—Ç—ã –∏ –Ω–æ—Ä–º—É
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
from PIL import Image
import os
import matplotlib.pyplot as plt
from pathlib import Path

def analyze_image_distributions():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
    print("üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
    
    def get_image_stats(image_path):
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        try:
            img = Image.open(image_path)
            if img.mode != 'RGB':
                img = img.convert('RGB')
            img_array = np.array(img)
            
            return {
                'mean': np.mean(img_array),
                'std': np.std(img_array),
                'min': np.min(img_array),
                'max': np.max(img_array),
                'shape': img_array.shape
            }
        except:
            return None
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–µ—Ñ–µ–∫—Ç—ã
    defect_stats = []
    test_positive_dir = "test/positive"
    if os.path.exists(test_positive_dir):
        for img_file in os.listdir(test_positive_dir):
            if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):
                img_path = os.path.join(test_positive_dir, img_file)
                stats = get_image_stats(img_path)
                if stats:
                    defect_stats.append(stats)
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–æ—Ä–º—É
    normal_stats = []
    test_negative_dir = "test/negative"
    if os.path.exists(test_negative_dir):
        for img_file in os.listdir(test_negative_dir):
            if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):
                img_path = os.path.join(test_negative_dir, img_file)
                stats = get_image_stats(img_path)
                if stats:
                    normal_stats.append(stats)
    
    print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:")
    print(f"   –î–µ—Ñ–µ–∫—Ç—ã: {len(defect_stats)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    print(f"   –ù–æ—Ä–º–∞: {len(normal_stats)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    if defect_stats and normal_stats:
        # –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
        defect_mean = np.mean([s['mean'] for s in defect_stats])
        normal_mean = np.mean([s['mean'] for s in normal_stats])
        
        defect_std = np.mean([s['std'] for s in defect_stats])
        normal_std = np.mean([s['std'] for s in normal_stats])
        
        print(f"\nüìà –°—Ä–µ–¥–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:")
        print(f"   –î–µ—Ñ–µ–∫—Ç—ã - —Å—Ä–µ–¥–Ω–µ–µ: {defect_mean:.1f}, std: {defect_std:.1f}")
        print(f"   –ù–æ—Ä–º–∞ - —Å—Ä–µ–¥–Ω–µ–µ: {normal_mean:.1f}, std: {normal_std:.1f}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–ª–∏—á–∏—è
        if abs(defect_mean - normal_mean) < 10:
            print("‚ö†Ô∏è –ü–†–û–ë–õ–ï–ú–ê: –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–µ—Ñ–µ–∫—Ç–æ–≤ –∏ –Ω–æ—Ä–º—ã –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–∏!")
            print("   –≠—Ç–æ –æ–±—ä—è—Å–Ω—è–µ—Ç, –ø–æ—á–µ–º—É –º–æ–¥–µ–ª—å –Ω–µ –º–æ–∂–µ—Ç –∏—Ö —Ä–∞–∑–ª–∏—á–∏—Ç—å")
        else:
            print("‚úÖ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ä–∞–∑–ª–∏—á–Ω—ã")

def test_simple_model():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø—Ä–æ—Å—Ç—É—é –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã"""
    print("\nüß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø—Ä–æ—Å—Ç—É—é –º–æ–¥–µ–ª—å...")
    
    # –°–æ–∑–¥–∞–µ–º –æ—á–µ–Ω—å –ø—Ä–æ—Å—Ç—É—é –º–æ–¥–µ–ª—å
    model = keras.Sequential([
        layers.Conv2D(16, (3, 3), activation="relu", input_shape=(224, 224, 3)),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(32, (3, 3), activation="relu"),
        layers.MaxPooling2D((2, 2)),
        layers.Flatten(),
        layers.Dense(64, activation="relu"),
        layers.Dense(1, activation="sigmoid")
    ])
    
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.01),  # –í—ã—Å–æ–∫–∏–π learning rate
        loss="binary_crossentropy",
        metrics=["accuracy"]
    )
    
    # –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö
    train_datagen = keras.preprocessing.image.ImageDataGenerator(
        rescale=1./255,
        horizontal_flip=True
    )
    
    train_generator = train_datagen.flow_from_directory(
        "train/",
        target_size=(224, 224),
        batch_size=32,
        class_mode="binary",
        shuffle=True
    )
    
    val_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)
    val_generator = val_datagen.flow_from_directory(
        "val/",
        target_size=(224, 224),
        batch_size=32,
        class_mode="binary",
        shuffle=False
    )
    
    # –û–±—É—á–∞–µ–º –Ω–∞ 5 —ç–ø–æ—Ö
    print("üöÄ –û–±—É—á–∞–µ–º –ø—Ä–æ—Å—Ç—É—é –º–æ–¥–µ–ª—å –Ω–∞ 5 —ç–ø–æ—Ö...")
    history = model.fit(
        train_generator,
        epochs=5,
        validation_data=val_generator,
        verbose=1
    )
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º
    test_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)
    test_generator = test_datagen.flow_from_directory(
        "test/",
        target_size=(224, 224),
        batch_size=1,
        class_mode="binary",
        shuffle=False
    )
    
    # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    predictions = model.predict(test_generator, verbose=0)
    true_labels = test_generator.classes
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    defect_predictions = predictions[true_labels == 1]
    normal_predictions = predictions[true_labels == 0]
    
    print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ—Å—Ç–æ–π –º–æ–¥–µ–ª–∏:")
    print(f"   –î–µ—Ñ–µ–∫—Ç—ã - —Å—Ä–µ–¥–Ω–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {np.mean(defect_predictions):.3f}")
    print(f"   –ù–æ—Ä–º–∞ - —Å—Ä–µ–¥–Ω–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {np.mean(normal_predictions):.3f}")
    print(f"   –†–∞–∑–Ω–∏—Ü–∞: {abs(np.mean(defect_predictions) - np.mean(normal_predictions)):.3f}")
    
    if abs(np.mean(defect_predictions) - np.mean(normal_predictions)) < 0.1:
        print("‚ö†Ô∏è –ü–†–û–ë–õ–ï–ú–ê: –ú–æ–¥–µ–ª—å –Ω–µ –º–æ–∂–µ—Ç —Ä–∞–∑–ª–∏—á–∏—Ç—å –∫–ª–∞—Å—Å—ã!")
        print("   –≠—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –ø—Ä–æ–±–ª–µ–º—É —Å –¥–∞–Ω–Ω—ã–º–∏")
    else:
        print("‚úÖ –ú–æ–¥–µ–ª—å –º–æ–∂–µ—Ç —Ä–∞–∑–ª–∏—á–∏—Ç—å –∫–ª–∞—Å—Å—ã")

def check_data_quality():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö"""
    print("\nüîç –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤
    train_pos = len(list(Path("train/positive").glob("*.jpg"))) if Path("train/positive").exists() else 0
    train_neg = len(list(Path("train/negative").glob("*.jpg"))) if Path("train/negative").exists() else 0
    val_pos = len(list(Path("val/positive").glob("*.jpg"))) if Path("val/positive").exists() else 0
    val_neg = len(list(Path("val/negative").glob("*.jpg"))) if Path("val/negative").exists() else 0
    test_pos = len(list(Path("test/positive").glob("*.jpg"))) if Path("test/positive").exists() else 0
    test_neg = len(list(Path("test/negative").glob("*.jpg"))) if Path("test/negative").exists() else 0
    
    print(f"üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:")
    print(f"   Train: {train_pos} –¥–µ—Ñ–µ–∫—Ç–æ–≤, {train_neg} –Ω–æ—Ä–º–∞")
    print(f"   Val: {val_pos} –¥–µ—Ñ–µ–∫—Ç–æ–≤, {val_neg} –Ω–æ—Ä–º–∞")
    print(f"   Test: {test_pos} –¥–µ—Ñ–µ–∫—Ç–æ–≤, {test_neg} –Ω–æ—Ä–º–∞")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–ª–∞–Ω—Å
    total_pos = train_pos + val_pos + test_pos
    total_neg = train_neg + val_neg + test_neg
    
    if total_pos > 0 and total_neg > 0:
        ratio = total_pos / total_neg
        print(f"   –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –¥–µ—Ñ–µ–∫—Ç—ã/–Ω–æ—Ä–º–∞: {ratio:.2f}")
        
        if ratio < 0.5 or ratio > 2.0:
            print("‚ö†Ô∏è –ü–†–û–ë–õ–ï–ú–ê: –°–∏–ª—å–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤!")
        else:
            print("‚úÖ –ö–ª–∞—Å—Å—ã –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω—ã")

def suggest_solutions():
    """–ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã"""
    print("\nüí° –ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø –ü–û –†–ï–®–ï–ù–ò–Æ:")
    
    print("1. üîÑ –ü—Ä–æ–±–ª–µ–º–∞ —Å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏:")
    print("   - –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –º–æ–≥—É—Ç –±—ã—Ç—å —Å–ª–∏—à–∫–æ–º '–∏–¥–µ–∞–ª—å–Ω—ã–º–∏'")
    print("   - –ú–æ–¥–µ–ª—å –Ω–µ –º–æ–∂–µ—Ç —Ä–∞–∑–ª–∏—á–∏—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ –∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–µ—Ñ–µ–∫—Ç—ã")
    print("   - –†–ï–®–ï–ù–ò–ï: –î–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–µ—Ñ–µ–∫—Ç–æ–≤")
    
    print("\n2. üìä –ü—Ä–æ–±–ª–µ–º–∞ —Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π:")
    print("   - –°–ª–∏—à–∫–æ–º –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∏—Å–∫–∞–∂–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏")
    print("   - –†–ï–®–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—É—é –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é")
    
    print("\n3. üèóÔ∏è –ü—Ä–æ–±–ª–µ–º–∞ —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π:")
    print("   - –°–ª–∏—à–∫–æ–º —Å–ª–æ–∂–Ω–∞—è –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–æ–±—É—á–∞–µ—Ç—Å—è")
    print("   - –†–ï–®–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å transfer learning —Å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏")
    
    print("\n4. üéØ –ü—Ä–æ–±–ª–µ–º–∞ —Å loss function:")
    print("   - Binary crossentropy –º–æ–∂–µ—Ç –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç—å –¥–ª—è –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞")
    print("   - –†–ï–®–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å focal loss –∏–ª–∏ class weights")
    
    print("\n5. üìà –ü—Ä–æ–±–ª–µ–º–∞ —Å –¥–∞–Ω–Ω—ã–º–∏:")
    print("   - –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–µ—Ñ–µ–∫—Ç–æ–≤ –∏ –Ω–æ—Ä–º—ã —Å–ª–∏—à–∫–æ–º –ø–æ—Ö–æ–∂–∏")
    print("   - –†–ï–®–ï–ù–ò–ï: –°–æ–±—Ä–∞—Ç—å –±–æ–ª–µ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∏ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üîç –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–ª–µ–º—ã —Å –¥–∞–Ω–Ω—ã–º–∏")
    print("=" * 50)
    
    analyze_image_distributions()
    test_simple_model()
    check_data_quality()
    suggest_solutions()
    
    print("\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")

if __name__ == "__main__":
    main()


