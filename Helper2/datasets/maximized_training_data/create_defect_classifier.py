#!/usr/bin/env python3
"""
–°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ —Ç–∏–ø–æ–≤ –¥–µ—Ñ–µ–∫—Ç–æ–≤
–ú–æ–¥–µ–ª—å –±—É–¥–µ—Ç –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ç–∏–ø –¥–µ—Ñ–µ–∫—Ç–∞ –≤–º–µ—Å—Ç–æ –ø—Ä–æ—Å—Ç–æ "–¥–µ—Ñ–µ–∫—Ç/–Ω–æ—Ä–º–∞"
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint
import numpy as np
import json
from pathlib import Path
import os
import shutil

# –ö–ª–∞—Å—Å—ã –¥–µ—Ñ–µ–∫—Ç–æ–≤
DEFECT_CLASSES = {
    0: 'normal',           # –ù–æ—Ä–º–∞
    1: 'crack',           # –¢—Ä–µ—â–∏–Ω–∞
    2: 'stain',           # –ü—è—Ç–Ω–æ
    3: 'damage',          # –ü–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–µ
    4: 'glass_defect',    # –î–µ—Ñ–µ–∫—Ç —Å—Ç–µ–∫–ª–∞
    5: 'ceiling_issue'    # –ü—Ä–æ–±–ª–µ–º–∞ —Å –ø–æ—Ç–æ–ª–∫–æ–º
}

CLASS_NAMES = ['–ù–æ—Ä–º–∞', '–¢—Ä–µ—â–∏–Ω–∞', '–ü—è—Ç–Ω–æ', '–ü–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–µ', '–î–µ—Ñ–µ–∫—Ç —Å—Ç–µ–∫–ª–∞', '–ü—Ä–æ–±–ª–µ–º–∞ —Å –ø–æ—Ç–æ–ª–∫–æ–º']

def create_classifier_dataset():
    """–°–æ–∑–¥–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–∏–ø–æ–≤ –¥–µ—Ñ–µ–∫—Ç–æ–≤"""
    print("üîß –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–∏–ø–æ–≤ –¥–µ—Ñ–µ–∫—Ç–æ–≤...")
    
    # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞–ø–æ–∫
    classifier_dir = Path("defect_classifier_data")
    classifier_dir.mkdir(exist_ok=True)
    
    for class_name in CLASS_NAMES:
        (classifier_dir / class_name).mkdir(exist_ok=True)
    
    # –ö–æ–ø–∏—Ä—É–µ–º –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    normal_source = Path("test/negative")
    normal_dest = classifier_dir / "–ù–æ—Ä–º–∞"
    
    if normal_source.exists():
        for img_file in normal_source.glob("*.jpg"):
            shutil.copy2(img_file, normal_dest / img_file.name)
        print(f"‚úÖ –°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ {len(list(normal_dest.glob('*.jpg')))} –Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–µ—Ñ–µ–∫—Ç—ã –ø–æ —Ç–∏–ø–∞–º
    defect_source = Path("test/positive")
    if not defect_source.exists():
        print("‚ùå –ü–∞–ø–∫–∞ —Å –¥–µ—Ñ–µ–∫—Ç–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return
    
    defect_files = list(defect_source.glob("*.jpg"))
    print(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(defect_files)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–µ—Ñ–µ–∫—Ç–æ–≤")
    
    # –†–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ —Ç–∏–ø–∞–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–∑–≤–∞–Ω–∏–π —Ñ–∞–π–ª–æ–≤
    for img_file in defect_files:
        filename = img_file.name.lower()
        
        if 'crack' in filename or '—Ç—Ä–µ—â–∏–Ω' in filename:
            dest_class = "–¢—Ä–µ—â–∏–Ω–∞"
        elif 'stain' in filename or '–ø—è—Ç–Ω' in filename:
            dest_class = "–ü—è—Ç–Ω–æ"
        elif 'damage' in filename or '–ø–æ–≤—Ä–µ–∂–¥–µ–Ω' in filename:
            dest_class = "–ü–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–µ"
        elif 'glass' in filename or '—Å—Ç–µ–∫–ª' in filename:
            dest_class = "–î–µ—Ñ–µ–∫—Ç —Å—Ç–µ–∫–ª–∞"
        elif 'ceiling' in filename or '–ø–æ—Ç–æ–ª–æ–∫' in filename:
            dest_class = "–ü—Ä–æ–±–ª–µ–º–∞ —Å –ø–æ—Ç–æ–ª–∫–æ–º"
        else:
            # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é - –ø—è—Ç–Ω–æ (—Å–∞–º—ã–π —á–∞—Å—Ç—ã–π —Ç–∏–ø)
            dest_class = "–ü—è—Ç–Ω–æ"
        
        dest_path = classifier_dir / dest_class / img_file.name
        shutil.copy2(img_file, dest_path)
    
    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–ª–∞—Å—Å–∞–º:")
    for class_name in CLASS_NAMES:
        count = len(list((classifier_dir / class_name).glob("*.jpg")))
        print(f"   {class_name}: {count} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    return classifier_dir

def create_classifier_model(num_classes=6):
    """–°–æ–∑–¥–∞–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–∏–ø–æ–≤ –¥–µ—Ñ–µ–∫—Ç–æ–≤"""
    print("üèóÔ∏è –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞...")
    
    model = keras.Sequential([
        # –ü–µ—Ä–≤—ã–π –±–ª–æ–∫
        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
        layers.BatchNormalization(),
        layers.Conv2D(32, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.25),
        
        # –í—Ç–æ—Ä–æ–π –±–ª–æ–∫
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.BatchNormalization(),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.25),
        
        # –¢—Ä–µ—Ç–∏–π –±–ª–æ–∫
        layers.Conv2D(128, (3, 3), activation='relu'),
        layers.BatchNormalization(),
        layers.Conv2D(128, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.25),
        
        # –ß–µ—Ç–≤–µ—Ä—Ç—ã–π –±–ª–æ–∫
        layers.Conv2D(256, (3, 3), activation='relu'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.25),
        
        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
        layers.Flatten(),
        layers.Dense(512, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.5),
        layers.Dense(256, activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(num_classes, activation='softmax')  # 6 –∫–ª–∞—Å—Å–æ–≤
    ])
    
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    return model

def train_classifier():
    """–û–±—É—á–∞–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ç–∏–ø–æ–≤ –¥–µ—Ñ–µ–∫—Ç–æ–≤"""
    print("üöÄ –û–±—É—á–∞–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ç–∏–ø–æ–≤ –¥–µ—Ñ–µ–∫—Ç–æ–≤...")
    
    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    dataset_dir = create_classifier_dataset()
    
    # –°–æ–∑–¥–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö
    train_datagen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=20,
        width_shift_range=0.2,
        height_shift_range=0.2,
        horizontal_flip=True,
        zoom_range=0.2,
        shear_range=0.2,
        brightness_range=[0.8, 1.2],
        fill_mode='nearest',
        validation_split=0.2
    )
    
    # –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    train_generator = train_datagen.flow_from_directory(
        dataset_dir,
        target_size=(224, 224),
        batch_size=16,
        class_mode='categorical',
        subset='training',
        shuffle=True
    )
    
    # –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    val_generator = train_datagen.flow_from_directory(
        dataset_dir,
        target_size=(224, 224),
        batch_size=16,
        class_mode='categorical',
        subset='validation',
        shuffle=False
    )
    
    print(f"üìä –ö–ª–∞—Å—Å—ã: {train_generator.class_indices}")
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model = create_classifier_model(len(train_generator.class_indices))
    
    # Callbacks
    callbacks = [
        EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True, mode='max'),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7, verbose=1),
        ModelCheckpoint('defect_classifier_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)
    ]
    
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    history = model.fit(
        train_generator,
        epochs=20,
        validation_data=val_generator,
        callbacks=callbacks,
        verbose=1
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
    model.save('final_defect_classifier.h5')
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–ª–∞—Å—Å–∞—Ö
    class_info = {
        'class_indices': train_generator.class_indices,
        'class_names': CLASS_NAMES,
        'num_classes': len(train_generator.class_indices)
    }
    
    with open('defect_classifier_info.json', 'w', encoding='utf-8') as f:
        json.dump(class_info, f, ensure_ascii=False, indent=2)
    
    print("‚úÖ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –æ–±—É—á–µ–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω!")
    print(f"üìÅ –ú–æ–¥–µ–ª—å: final_defect_classifier.h5")
    print(f"üìÅ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è: defect_classifier_info.json")
    
    return model, train_generator.class_indices

def test_classifier():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä"""
    print("üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä...")
    
    try:
        model = keras.models.load_model('final_defect_classifier.h5')
        
        with open('defect_classifier_info.json', 'r', encoding='utf-8') as f:
            class_info = json.load(f)
        
        print("‚úÖ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ")
        print(f"üìä –ö–ª–∞—Å—Å—ã: {class_info['class_names']}")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö
        test_dir = Path("defect_classifier_data")
        if test_dir.exists():
            for class_name in class_info['class_names']:
                class_dir = test_dir / class_name
                if class_dir.exists():
                    test_files = list(class_dir.glob("*.jpg"))[:3]
                    print(f"\nüîç –¢–µ—Å—Ç–∏—Ä—É–µ–º {class_name}:")
                    
                    for img_file in test_files:
                        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                        img = keras.preprocessing.image.load_img(img_file, target_size=(224, 224))
                        img_array = keras.preprocessing.image.img_to_array(img)
                        img_array = np.expand_dims(img_array, axis=0) / 255.0
                        
                        # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º
                        predictions = model.predict(img_array, verbose=0)
                        predicted_class_idx = np.argmax(predictions[0])
                        confidence = predictions[0][predicted_class_idx] * 100
                        predicted_class = class_info['class_names'][predicted_class_idx]
                        
                        print(f"   {img_file.name}: {predicted_class} ({confidence:.1f}%)")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")

if __name__ == "__main__":
    print("üéØ –°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ —Ç–∏–ø–æ–≤ –¥–µ—Ñ–µ–∫—Ç–æ–≤")
    print("=" * 50)
    
    model, class_indices = train_classifier()
    test_classifier()
    
    print("\n‚úÖ –ì–æ—Ç–æ–≤–æ! –¢–µ–ø–µ—Ä—å –º–æ–¥–µ–ª—å –±—É–¥–µ—Ç –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ç–∏–ø—ã –¥–µ—Ñ–µ–∫—Ç–æ–≤.")


